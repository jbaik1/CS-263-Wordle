{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from wordle import Wordle\n",
    "from prompting import Prompt_Helper\n",
    "from run_models import extract_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944361d2c791434a8330f7217acc0dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config model + quantization\n",
    "llama3_8b = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_8bit=True\n",
    "# )\n",
    "# bnb_config = None\n",
    "\n",
    "# Load in NF4 to fit 15GB GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "   llama3_8b,\n",
    "   padding_side=\"right\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llama3_8b, \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\", )\n",
    "\n",
    "# Set pad token to eos token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-04 18:03:29 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dangnth/anaconda3/envs/wordle/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-04 18:03:31 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-04 18:03:34 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-04 18:03:35 gpu_executor.py:83] # GPU blocks: 10821, # CPU blocks: 2048\n",
      "INFO 06-04 18:03:37 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-04 18:03:37 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-04 18:03:41 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "llama3_8b = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "gpu_memory_utilization = 0.8\n",
    "\n",
    "llm = LLM(\n",
    "    model=llama3_8b, \n",
    "    tokenizer=llama3_8b, \n",
    "    tensor_parallel_size=torch.cuda.device_count(), \n",
    "    dtype='bfloat16', \n",
    "    trust_remote_code=True, \n",
    "    gpu_memory_utilization=gpu_memory_utilization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()\n",
    "# Set pad token to eos token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Wordle game and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Wordle()\n",
    "p = Prompt_Helper()\n",
    "\n",
    "# Get a few-shot instruction\n",
    "conversation = p.load_few_shot_examples()\n",
    "# print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.19s/it, Generation Speed: 21.09 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play another game.\n",
      "\n",
      "My random initial guess is: \"FABLE\"\n",
      "\n",
      "Please provide the feedback for each letter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:05<00:00,  5.69s/it, Generation Speed: 36.36 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess:\n",
      "{ \n",
      "    \"guess: \"TABLE\",\n",
      "    \"reasoning\" : \"\n",
      "        The list of previously guessed words are: ['FABLE']\n",
      "        The list of valid letters are: ['A', 'L', 'E']\n",
      "        The list of invalid letters are: ['F', 'B']\n",
      "\n",
      "        The previous guess was: FABLE\n",
      "        Given the previous feedback, we know that\n",
      "        1. The letter at position 1 (F) was not in the string \n",
      "        2. The letter at position 2 (A) was in the string but in the wrong position \n",
      "        3. The letter at position 3 (B) was not in the string \n",
      "        4. The letter at position 4 (L) was in the string but in the wrong position \n",
      "        5. The letter at position 5 (E) was in the string but in the wrong position \n",
      "\n",
      "        Improving on this feedback,\n",
      "        the new guess is TABLE\n",
      "        \"\n",
      "        \n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:05<00:00,  5.94s/it, Generation Speed: 36.02 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess:\n",
      "{ \n",
      "    \"guess: \"GABLE\",\n",
      "    \"reasoning\" : \"\n",
      "        The list of previously guessed words are: ['FABLE', 'TABLE']\n",
      "        The list of valid letters are: ['A', 'L', 'E']\n",
      "        The list of invalid letters are: ['F', 'B', 'T']\n",
      "\n",
      "        The previous guess was: TABLE\n",
      "        Given the previous feedback, we know that\n",
      "        1. The letter at position 1 (T) was not in the string \n",
      "        2. The letter at position 2 (A) was in the string but in the wrong position \n",
      "        3. The letter at position 3 (B) was not in the string \n",
      "        4. The letter at position 4 (L) was in the string but in the wrong position \n",
      "        5. The letter at position 5 (E) was in the string but in the wrong position \n",
      "\n",
      "        Improving on this feedback,\n",
      "        the new guess is GABLE\n",
      "        \"\n",
      "        \n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:06<00:00,  6.19s/it, Generation Speed: 35.89 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess:\n",
      "{ \n",
      "    \"guess: \"CABLE\",\n",
      "    \"reasoning\" : \"\n",
      "        The list of previously guessed words are: ['FABLE', 'TABLE', 'GABLE']\n",
      "        The list of valid letters are: ['A', 'L', 'E']\n",
      "        The list of invalid letters are: ['F', 'B', 'T', 'G']\n",
      "\n",
      "        The previous guess was: GABLE\n",
      "        Given the previous feedback, we know that\n",
      "        1. The letter at position 1 (G) was not in the string \n",
      "        2. The letter at position 2 (A) was in the string but in the wrong position \n",
      "        3. The letter at position 3 (B) was not in the string \n",
      "        4. The letter at position 4 (L) was in the string but in the wrong position \n",
      "        5. The letter at position 5 (E) was in the string but in the wrong position \n",
      "\n",
      "        Improving on this feedback,\n",
      "        the new guess is CABLE\n",
      "        \"\n",
      "        \n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:06<00:00,  6.42s/it, Generation Speed: 35.66 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess:\n",
      "{ \n",
      "    \"guess: \"SABLE\",\n",
      "    \"reasoning\" : \"\n",
      "        The list of previously guessed words are: ['FABLE', 'TABLE', 'GABLE', 'CABLE']\n",
      "        The list of valid letters are: ['A', 'L', 'E']\n",
      "        The list of invalid letters are: ['F', 'B', 'T', 'G', 'C']\n",
      "\n",
      "        The previous guess was: CABLE\n",
      "        Given the previous feedback, we know that\n",
      "        1. The letter at position 1 (C) was not in the string \n",
      "        2. The letter at position 2 (A) was in the string but in the wrong position \n",
      "        3. The letter at position 3 (B) was not in the string \n",
      "        4. The letter at position 4 (L) was in the string but in the wrong position \n",
      "        5. The letter at position 5 (E) was in the string but in the wrong position \n",
      "\n",
      "        Improving on this feedback,\n",
      "        the new guess is SABLE\n",
      "        \"\n",
      "        \n",
      "}\n",
      "{'role': 'system', 'content': '5 guesses exhausted! The correct answer is equal'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "victory = False\n",
    "\n",
    "for turn_idx in range(5):\n",
    "    # input_ids = tokenizer.apply_chat_template(\n",
    "    #     conversation,\n",
    "    #     add_generation_prompt=True,\n",
    "    #     return_tensors=\"pt\"\n",
    "    # ).to(model.device)\n",
    "\n",
    "    # terminators = [\n",
    "    #     tokenizer.eos_token_id,\n",
    "    #     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    # ]\n",
    "\n",
    "    # outputs = model.generate(\n",
    "    #     input_ids,\n",
    "    #     max_new_tokens=500,\n",
    "    #     eos_token_id=terminators,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     do_sample=True,\n",
    "    #     temperature=0.3,\n",
    "    #     top_p=0.9,\n",
    "    # )\n",
    "    # response = outputs[0][input_ids.shape[-1]:]\n",
    "    # response_str = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    stop_tokens = [\"Question:\", \"Question\", \"USER:\", \"USER\", \"ASSISTANT:\", \"ASSISTANT\", \"Instruction:\", \"Instruction\", \"Response:\", \"Response\", \"### Instruction\"]\n",
    "    sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=500, stop=stop_tokens)\n",
    "    inputs = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "    outputs = llm.generate(inputs, sampling_params)\n",
    "    # print(outputs)\n",
    "    response_str = outputs[0].outputs[0].text\n",
    "    print(response_str)\n",
    "    \n",
    "    guess_eval = \"\"\n",
    "    try:\n",
    "        guess_str = extract_guess(response_str)\n",
    "        guess_eval = game.turn(guess_str)\n",
    "        user_response = p.jb_feedback(guess_str, guess_eval, turn_idx)[\"feedback\"]\n",
    "        conversation.extend([\n",
    "            {'role': 'assistant', 'content': response_str},\n",
    "            {'role': 'user', 'content': user_response}\n",
    "        ])\n",
    "    except ValueError as error:\n",
    "        print(\"VALUE ERROR GAME\", error)\n",
    "        print(response_str)\n",
    "        # conversation.extend([\n",
    "        #     {'role': 'assistant', 'content': response_str},\n",
    "        #     {'role': 'user', 'content': str(e)}\n",
    "        # ])\n",
    "        break\n",
    "    except IndexError as error:\n",
    "        print(\"INDEX ERROR GAME\", error)\n",
    "        print(response_str)\n",
    "        break\n",
    "\n",
    "\n",
    "    if guess_eval == 'GGGGG':\n",
    "        conversation.append({'role': 'system', 'content': f\"Correct! The answer is {game.get_answer()}\"})\n",
    "        victory = True\n",
    "        break\n",
    "\n",
    "if not victory:\n",
    "    conversation.append({'role': 'system', 'content': f\"5 guesses exhausted! The correct answer is {game.get_answer()}\"})\n",
    "\n",
    "print(conversation[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
